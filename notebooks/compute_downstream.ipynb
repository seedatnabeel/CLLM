{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for covid with seed 0 and ns 10\n",
      "Applying curation and store subsets...\n",
      "Fitting downstream models on the different stored datasets...\n",
      "Computing for covid with seed 1 and ns 10\n",
      "Applying curation and store subsets...\n",
      "Fitting downstream models on the different stored datasets...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.utils import *\n",
    "from src.curation import *\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "n_synthetic = 1000\n",
    "retrain = False\n",
    "nest=100\n",
    "downstream = 'xgb'\n",
    "curation_ythresh=0.2\n",
    "curation_xthresh = 0.15\n",
    "temp = 0.9\n",
    "\n",
    "# Factors to evaluate\n",
    "seeds = [0,1,2,3,4,5,6,7,8,9]\n",
    "n_samples = [10,20,50,100] \n",
    "datasets = ['covid', 'adult', 'seer', 'cutract', 'maggic']\n",
    "\n",
    "ylabel_map = {'covid': \"is_dead\",\n",
    "            \"adult\": \"salary\",\n",
    "            \"compas\": \"y\",\n",
    "            \"seer\": \"mortCancer\",\n",
    "            \"cutract\":\"mortCancer\",\n",
    "            \"maggic\": \"death_all\",\n",
    "            \"support\": \"death\",\n",
    "            'bio': 'y', \n",
    "            'higgs': \"y\",\n",
    "            'drug': 'target'\n",
    "            }\n",
    "\n",
    "for dataset in datasets:  \n",
    "\n",
    "    for ns in n_samples:\n",
    "        performance_all = []\n",
    "        acc_list_all = []\n",
    "        auc_list_all = []\n",
    "        f1_list_all = []\n",
    "\n",
    "        acc_clf_all = []\n",
    "        auc_clf_all = []\n",
    "        f1_clf_all = []\n",
    "\n",
    "        results_plot_all = []\n",
    "        results_data_all = []\n",
    "        subset_dict_all = []\n",
    "         \n",
    "        for seed in seeds:\n",
    "            try:\n",
    "                    print(f\"Computing for {dataset} with seed {seed} and ns {ns}\")\n",
    "\n",
    "                    dfs_dicts = {}\n",
    "                    results = {}\n",
    "                    subset_dict = {}\n",
    "\n",
    "                    ylabel =  ylabel_map[dataset]\n",
    "\n",
    "                    # Get the GPT-4 Generated data\n",
    "                    gpt_model = 'gpt4'\n",
    "                    tmp_df = process_gpt(dataset=dataset, n_synthetic=n_synthetic, temp=temp, gpt_model=gpt_model, ns=ns, seed=seed)\n",
    "                    tmp_df = tmp_df.reset_index(drop=True)\n",
    "                    results['gpt4'] =  {\"X\": tmp_df.drop(columns=['target']), \"y\": tmp_df['target'], \"df\": tmp_df}\n",
    "\n",
    "                    # Get the GPT-3.5 Generated data\n",
    "                    gpt_model = 'gpt3'\n",
    "                    tmp_df = process_gpt(dataset=dataset, n_synthetic=n_synthetic, temp=temp, gpt_model=gpt_model, ns=ns, seed=seed)\n",
    "                    tmp_df = tmp_df.reset_index(drop=True)\n",
    "                    results[gpt_model] =  {\"X\": tmp_df.drop(columns=['target']), \"y\": tmp_df['target'], \"df\": tmp_df}\n",
    "     \n",
    "                    # Extract the Great datasets\n",
    "                    filename = f\"../save_dfs/great_pipeline_{dataset}_{seed}_{ns}.pickle\"\n",
    "                    with open(filename, 'rb') as f:\n",
    "                                great_df = pickle.load(f)\n",
    "\n",
    "                    tmp_df = great_df['great']['X']\n",
    "                    tmp_df['target'] = great_df['great']['y']\n",
    "                    tmp_df = tmp_df.reset_index(drop=True)\n",
    "                    results['great'] =  {\"X\": tmp_df.drop(columns=['target']), \"y\": tmp_df['target'], \"df\": tmp_df}\n",
    "  \n",
    "\n",
    "                    # Get all the baseline results\n",
    "                    filename = f\"../save_dfs/pipeline_{dataset}_{seed}_{ns}.pickle\"\n",
    "                    with open(filename, 'rb') as f:\n",
    "                            df = pickle.load(f)\n",
    "\n",
    "                    # Extract each baseline method, as well as Dorig, Doracle, Dtest\n",
    "                    for model in list(df.keys()):\n",
    "\n",
    "                        if 'great' in model:\n",
    "                            continue\n",
    "\n",
    "                        tmp_df = df[model]['X']\n",
    "                        tmp_df['target'] = df[model]['y']\n",
    "\n",
    "                        # reset index\n",
    "                        tmp_df = tmp_df.reset_index(drop=True)\n",
    "\n",
    "                        if model == 'Original':\n",
    "                            X_train_orig, y_train_orig = tmp_df.drop(columns=['target']), tmp_df['target']\n",
    "                        elif model == 'Oracle':\n",
    "                            X_oracle, y_oracle = tmp_df.drop(columns=['target']), tmp_df['target']\n",
    "                        elif model == 'Test':\n",
    "                            X_test, y_test = tmp_df.drop(columns=['target']), tmp_df['target']\n",
    "\n",
    "                        if model!='Test':\n",
    "                            results[model] =  {\"X\": tmp_df.drop(columns=['target']), \"y\": tmp_df['target'], \"df\": tmp_df}\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    acc_list = []\n",
    "                    auc_list = []\n",
    "                    recall_list = []\n",
    "                    precision_list = []\n",
    "                    f1_list = []\n",
    "                    model_names = []\n",
    "                    acc_per_clf = {}\n",
    "                    auc_per_clf = {}\n",
    "                    f1_per_clf = {}\n",
    "\n",
    "\n",
    "                    print('Applying curation and store subsets...')\n",
    "                    results_models = list(results.keys())\n",
    "\n",
    "                    # enrich results dict\n",
    "                    for model in results_models:\n",
    "                        if model=='Original' or model=='Oracle':\n",
    "                            continue\n",
    "\n",
    "                        X_eval, y_eval = results[model]['X'], results[model]['y']\n",
    "\n",
    "                        df_sample = deepcopy(X_eval)\n",
    "                        df_sample['target'] = y_eval\n",
    "                        if len(df_sample)>1000:\n",
    "                            df_sample = df_sample.sample(n=1000, random_state=seed,).reset_index(drop=True)\n",
    "                        X_eval, y_eval = df_sample.drop(columns=['target']), df_sample['target']\n",
    "\n",
    "                        # CLLM Curation Mechanism\n",
    "                        easy_train, ambig_train, hard_train, dataiq_model  = data_centric_curation(X_train_orig = X_train_orig, \n",
    "                                    y_train_orig= y_train_orig, \n",
    "                                    X_check = X_eval,\n",
    "                                    y_check = y_eval,\n",
    "                                    retrain=retrain,\n",
    "                                    nest=nest,\n",
    "                                    curation_ythresh=curation_ythresh,\n",
    "                                    curation_xthresh=curation_xthresh,\n",
    "                                    )\n",
    "                        \n",
    "                        df_save = deepcopy(X_eval)\n",
    "                        df_save['target'] = y_eval\n",
    "\n",
    "                        # Store data subsets\n",
    "                        subset_dict[model] = {\"easy\": df_save.iloc[easy_train,:], \"ambig\": df_save.iloc[ambig_train,:], \"hard\": df_save.iloc[hard_train,:], \"easy_ambig\": df_save.iloc[np.concatenate((easy_train, ambig_train)),:], \"all\": df_save, \"easy_ids\": easy_train, \"ambig_ids\": ambig_train, \"hard_ids\": hard_train, \"easy_ambig_ids\": np.concatenate((easy_train, ambig_train))}\n",
    "                        \n",
    "                        # results[f'{model}_easy'] = {\"X\": X_eval.iloc[easy_train,:], \"y\": y_eval[easy_train], \"df\": df_save.iloc[easy_train,:]}\n",
    "                        # results[f'{model}_ambig'] = {\"X\": X_eval.iloc[ambig_train,:], \"y\": y_eval[ambig_train], \"df\": df_save.iloc[ambig_train,:]}\n",
    "                        \n",
    "                        # The curated is after we remove 'Hard' synthetic samples\n",
    "                        results[f'{model}_curated'] = {\"X\": X_eval.iloc[np.concatenate((easy_train, ambig_train)),:], \"y\": y_eval[np.concatenate((easy_train, ambig_train))], \"df\": df_save.iloc[np.concatenate((easy_train, ambig_train)),:]}\n",
    "\n",
    "              \n",
    "                    print('Fitting downstream models on the different stored datasets...')\n",
    "                    for idx, model in enumerate(list(results.keys())):\n",
    "                        \n",
    "                        X_eval, y_eval = results[model]['X'], results[model]['y']\n",
    "                        clf1 = XGBClassifier(n_estimators=nest, random_state=seed)\n",
    "                        clf2 = RandomForestClassifier(n_estimators=nest, random_state=seed)\n",
    "                        clf3 = LogisticRegression(random_state=seed)\n",
    "                        clf4 = DecisionTreeClassifier(random_state=seed)\n",
    "  \n",
    "                        X_eval, y_eval = results[model]['X'], results[model]['y']\n",
    "                        try:\n",
    "                            scaler = preprocessing.StandardScaler().fit(X_eval)\n",
    "                        except:\n",
    "                             print(model)\n",
    "\n",
    "                        # Perform model evaluation\n",
    "                        acc1, rec1, prec1, f11, auc1, clf1 = evaluate_model(scaler.transform(X_eval), y_eval, scaler.transform(X_test), y_test, clf1)\n",
    "                        acc2, rec2, prec2, f12, auc2, clf2 = evaluate_model(scaler.transform(X_eval), y_eval, scaler.transform(X_test), y_test, clf2)\n",
    "                        acc3, rec3, prec3, f13, auc3, clf3 = evaluate_model(scaler.transform(X_eval), y_eval, scaler.transform(X_test), y_test, clf3)\n",
    "                        acc4, rec4, prec4, f14, auc4, clf4 = evaluate_model(scaler.transform(X_eval), y_eval, scaler.transform(X_test), y_test, clf4)\n",
    "\n",
    "                        acc_per_clf[model] = {'xgb': acc1, 'rf': acc2, 'lr': acc3, 'dt': acc4}\n",
    "                        auc_per_clf[model] = {'xgb': auc1, 'rf': auc2, 'lr': auc3, 'dt': auc4}\n",
    "                        f1_per_clf[model] = {'xgb': f11, 'rf': f12, 'lr': f13, 'dt': f14}\n",
    "\n",
    "                        acc = np.mean([acc1, acc2, acc3])\n",
    "                        rec = np.mean([rec1, rec2, rec3])\n",
    "                        prec = np.mean([prec1, prec2, prec3])\n",
    "                        f1 = np.mean([f11, f12, f13])\n",
    "                        auc = np.mean([auc1, auc2, auc3])\n",
    "                        \n",
    "                        acc_list.append(acc)\n",
    "                        recall_list.append(rec)\n",
    "                        precision_list.append(prec)\n",
    "                        f1_list.append(f1)\n",
    "                        auc_list.append(auc)\n",
    "                        model_names.append(model)\n",
    "\n",
    "                        if model!='Original' or model!='Oracle':\n",
    "\n",
    "                            if model=='Oracle':\n",
    "                                oracle_idx = idx\n",
    "\n",
    "                            continue\n",
    "\n",
    "                    # Store results\n",
    "                    performance_dict = {'Accuracy': acc_list, 'AUC': auc_list, 'Group': model_names, 'Recall': recall_list, 'Precision': precision_list, 'F1': f1_list, 'Dataset': dataset, 'Seed': seed, 'acc_clf': acc_per_clf, 'auc_clf': auc_per_clf, 'f1_clf': f1_per_clf}\n",
    "                    performance_all.append(performance_dict)\n",
    "\n",
    "                    acc_list_all.append(acc_list)\n",
    "                    auc_list_all.append(auc_list)\n",
    "                    f1_list_all.append(f1_list)\n",
    "                    acc_clf_all.append(acc_per_clf)\n",
    "                    auc_clf_all.append(auc_per_clf)\n",
    "                    f1_clf_all.append(f1_per_clf)\n",
    "\n",
    "                    results_data_all.append(results)\n",
    "                    subset_dict_all.append(subset_dict)\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                print(traceback.format_exc())\n",
    "                print('Error in fitting models: ', e)\n",
    "                continue    \n",
    "        \n",
    "            \n",
    "            try:\n",
    "\n",
    "                acc_list = np.mean(np.array(acc_list_all), axis=0)\n",
    "                auc_list = np.mean(np.array(auc_list_all), axis=0)\n",
    "                f1_list = np.mean(np.array(f1_list_all), axis=0)\n",
    "\n",
    "                df = pd.DataFrame({'Accuracy': acc_list, 'AUC': auc_list, 'Group': model_names})\n",
    "                df = pd.melt(df, id_vars=['Group'], value_vars=['Accuracy', 'AUC'])\n",
    "                df.columns = ['Group', 'Metric', 'Score']\n",
    "\n",
    "                results_plot_all.append(df)\n",
    "\n",
    "                # create a dict to store all the results\n",
    "                all_results = {'model_order': list(results.keys()),\n",
    "                                'performance_all': performance_all, \n",
    "                                'acc_list_all': acc_list_all, \n",
    "                                'auc_list_all': auc_list_all, \n",
    "                                'f1_list_all': f1_list_all,\n",
    "                                'results_plot_all': results_plot_all, \n",
    "                                'results_data_all': results_data_all, \n",
    "                                'subset_dict_all': subset_dict_all, \n",
    "                                }\n",
    "                \n",
    "                # pickle all the results to results summary folder\n",
    "                filename = f\"../results_summary/results_summary_{dataset}_{ns}.pickle\"\n",
    "\n",
    "                with open(filename, 'wb') as f:\n",
    "                    pickle.dump(all_results, f)\n",
    "\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                print(traceback.format_exc())\n",
    "                print('Error in computing performance metrics: ', e)\n",
    "                continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
